{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using okgrade\n",
    "This notebook demonstrates how to use [okgrade](http://okgrade.readthedocs.io/en/latest/). okgrade is a Python library that allows instructors to grade students' Jupyter notebooks or Python source files. Imagine you're an instructor and have assigned a notebook to complete for homework. Perhaps there are 10 questions in that notebook and 100 students in the course. Rather than having to run each student's notebook one by one and visually inspecting the answers, okgrade will do that for you. You can find more information about okgrade, including installation instructions, at the link above.\n",
    "\n",
    "To use okgrade, you only need to do two things:\n",
    "\n",
    "- Write tests for each question\n",
    "- Use okgrade's `grade` function to run your tests\n",
    "\n",
    "### A quick working example\n",
    "\n",
    "Before we go into detail about these two steps, let's see a quick working example. Imagine this notebook is one of your homework notebooks, and you want to test the students' ability to implement the [Euclidean algorithm](https://en.wikipedia.org/wiki/Euclidean_algorithm). You ask them the following question:\n",
    "\n",
    "#### Question 1\n",
    "_Write a function called `gcd` that takes two positive integers and returns their greatest common divisor._\n",
    "\n",
    "Hopefully, your students define a function something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def gcd(a, b):\n",
    "    while b:\n",
    "        a, b = b, a%b\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've written tests for this question already, so all you need to do is run the following cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (grader.py, line 72)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/redwards/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2963\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-4-f80e61aa2245>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from okgrade import grade\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/redwards/anaconda/lib/python3.5/site-packages/okgrade/__init__.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from okgrade.grader import grade\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/redwards/anaconda/lib/python3.5/site-packages/okgrade/grader.py\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    return TestResult(1, f\"{test_file_path}: All tests passed!\")\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from okgrade import grade\n",
    "grade('tests/q1.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - all the tests passed! If, however, a student gives a wrong answer, the tests will catch that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wrong answer\n",
    "def gcd(a, b):\n",
    "    while b:\n",
    "        a, b = b, a%b\n",
    "    return a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Test Euclidean algorithm 1 failed!\n",
       "\n",
       "Test code:\n",
       "\n",
       "&gt;&gt;&gt; gcd(200, 15)\n",
       "5\n",
       "\n",
       "\n",
       "Test result:\n",
       "Trying:\n",
       "    gcd(200, 15)\n",
       "Expecting:\n",
       "    5\n",
       "**********************************************************************\n",
       "Line 2, in Euclidean algorithm 1\n",
       "Failed example:\n",
       "    gcd(200, 15)\n",
       "Expected:\n",
       "    5\n",
       "Got:\n",
       "    6\n",
       "</pre>"
      ],
      "text/plain": [
       "<okgrade.result.TestResult at 0x104d1c4a8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade('tests/q1.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing tests\n",
    "The way okgrade grades is by running tests on students' code to see if it does what is expected. In the example above, we knew (from pen and paper calcuations) that the greatest common divisor of 200 and 15 is 5. If students correctly implemented the Euclidean algorithm, then when we call their `gcd` function with 200 and 15 as arguments, it should return 5. If it does, the student's code passes that test. If not, it fails. As the instructor, you have to decide what tests are necessary and sufficient to test students' code. Once you've done that, you have to actually write the tests.\n",
    "\n",
    "We write tests in Python files. These files are separate from the notebook or Python file that's being graded. Tests are normally kept in a folder called `tests`, but you can put them wherever you want. You can also call them whatever you want, but a common convention is \"q1.py\" for \"question 1\".\n",
    "\n",
    "At the moment, tests are written in a particular format for historical reasons (it's a subset of the [OK test format](https://github.com/Cal-CS-61A-Staff/ok-client/wiki/ok_test)). Basically, the only thing the test file should do is assign a Python dictionary to the name `test`. That dictionary must follow a particular structure, so in practice it's usually best to just copy a template test file and change a few things. As an example, here's the entire contents of `tests/q1.py` from above:\n",
    "\n",
    "```\n",
    "test = {\n",
    "    'name': 'Euclidean algorithm',\n",
    "    'suites': [\n",
    "        {\n",
    "            'cases': [\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> gcd(200, 15)\n",
    "                    5\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> gcd(200, 14)\n",
    "                    2\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> gcd(19, 201)\n",
    "                    1\n",
    "                    \"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "The important parts are the name right at the top ('Euclidean algorithm'), and the actual three tests. Everything else is boilerplate code. The tests are stored as raw strings following the [doctest](https://docs.python.org/3/library/doctest.html) format. The doctest format simulates the standard Python interpreter. You write the code you want to evaluate after the `>>>` and then the expected result on the following line. Here's a template test file, just replace the bits in all caps:\n",
    "\n",
    "\n",
    "```\n",
    "test = {\n",
    "    'name': 'THE NAME OF THE TEST/QUESTION',\n",
    "    'suites': [\n",
    "        {\n",
    "            'cases': [\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> CODE THAT YOU WANT TO TEST\n",
    "                    EXPECTED RESULT\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> CODE THAT YOU WANT TO TEST\n",
    "                    EXPECTED RESULT\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> CODE THAT YOU WANT TO TEST\n",
    "                    EXPECTED RESULT\n",
    "                    \"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "There are a couple of points worth noting. First, we have three tests in this file but you can have as many as you want. Just copy the inner most dictionary (shown again below) as many times as you need.\n",
    "\n",
    "```\n",
    "{\n",
    "                    'code': r\"\"\"\n",
    "                    >>> CODE THAT YOU WANT TO TEST\n",
    "                    EXPECTED RESULT\n",
    "                    \"\"\"\n",
    "                }\n",
    "```\n",
    "\n",
    "Second, each test can test as many lines as you want. For example, this is a valid test:\n",
    "\n",
    "```\n",
    "{\n",
    "                    'code': r\"\"\"\n",
    "                    >>> gcd(19, 201)\n",
    "                    1\n",
    "                    >>> gcd(200, 16)\n",
    "                    2\n",
    "                    >>> callable(gcd)\n",
    "                    True\n",
    "                    \"\"\"\n",
    "                }\n",
    "```\n",
    "\n",
    "Third, these tests have access to the global namespace of the student's notebook being graded. So we can test any names that the student uses. This is why we never defined `gcd` ourselves, we're using the `gcd` that the student defined. If we asked a student to assign their answer to a question in a variable called `my_answer`, we can just test that their `my_answer` has the value we expect.\n",
    "\n",
    "Fourth, as you saw above in the working example, when a test fails, it prints the simulated interactive session that didn't pass. We can use this to display helpful feedback to students by adding comments above the test that explain why it failed.\n",
    "\n",
    "```\n",
    "{\n",
    "                    'code': r\"\"\"\n",
    "                    >>> # It looks like you forgot to store your answer in a variable called 'my_answer'!\n",
    "                    >>> # Did you spell it as 'my_answer' exactly?\n",
    "                    >>> 'my_answer' in vars()\n",
    "                    True\n",
    "                    \"\"\"\n",
    "                }\n",
    "```\n",
    "\n",
    "Fifth, if you've been using [okpy](https://okpy.org/) and are used to writing tests with the full OK test format, here are some important restrictions on the test format used in okgrade. At the moment, all tests must be worth one point each. It's easiest if you don't specify any points yourself and let okgrade assume one point for each test. You also can't use any set-up or tear-down code.\n",
    "\n",
    "### okgrade's `grade` function\n",
    "\n",
    "Now that we know how to write tests, let's look at the `grade` function. Imagine that you've assigned a homework on regular expressions. Perhaps you start off with a question like this:\n",
    "\n",
    "#### Question 2\n",
    "*Write a regular expression pattern to validate a US phone number that consists of three digits, a dash ('-'), three more digits, a dash, and then four digits. Store the pattern (as a string) in a variable called `phone_number_pattern`.*\n",
    "\n",
    "Hopefully your students write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phone_number_pattern = r'\\d{3}-\\d{3}-\\d{4}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can test their answers with the following test, which is in \"tests/q2-basic.py\":\n",
    "```\n",
    "{\n",
    "                    'code': r\"\"\"\n",
    "                    >>> import re\n",
    "                    >>> test_string = 'Call me maybe? 510-123-4567'\n",
    "                    >>> match = re.search(phone_number_pattern, test_string)\n",
    "                    >>> bool(match)\n",
    "                    True\n",
    "                    \"\"\"\n",
    "                }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>tests/q2-basic.py: All tests passed!</pre>"
      ],
      "text/plain": [
       "<okgrade.result.TestResult at 0x104bf0518>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = grade('tests/q2-basic.py')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value of `grade` is a `TestResult`. We can ask instances of `TestResult` for the grade from that test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tests in 'tests/q2-basic.py' weren't thorough or helpful to students, so let's try the following tests in 'tests/q2-thorough.py:\n",
    "\n",
    "```\n",
    "               {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> # Make sure you assigned your answer to a variable called 'phone_number_pattern'.\n",
    "                    >>> 'phone_number_pattern' in vars()\n",
    "                    True\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> # Remember that phone_number_pattern should be a string.\n",
    "                    >>> isinstance(phone_number_pattern, str)\n",
    "                    True\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> # We're just looking for phone numbers without parentheses at the moment!\n",
    "                    >>> import re\n",
    "                    >>> test_string = \"Don't do anything fancy with parentheses: (510)-123-4567\"\n",
    "                    >>> match = re.search(phone_number_pattern, test_string)\n",
    "                    >>> bool(match)\n",
    "                    False\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    'code': r\"\"\"\n",
    "                    >>> # Are you sure your regular expression is looking for all ten digits?\n",
    "                    >>> import re\n",
    "                    >>> test_string = \"You shouldn't match on 123-4567 but you should on 510-123-4567\"\n",
    "                    >>> matches = re.findall(phone_number_pattern, test_string)\n",
    "                    >>> len(matches)\n",
    "                    1\n",
    "                    \"\"\"\n",
    "                }\n",
    "                \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = grade('tests/q2-thorough.py')\n",
    "result.grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current answer in `phone_number_pattern` passed all these tests, but perhaps another student misunderstood the question and submitted a compiled regular expression object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_number_pattern = re.compile(r'\\d{3}-\\d{3}-\\d{4}')\n",
    "result = grade('tests/q2-thorough.py')\n",
    "result.grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be finished..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "More examples to go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
